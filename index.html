<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Dataset, Language, Navigation, Perception, Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LaNMP: A Language-Conditioned Mobile Manipulation Benchmark for Autonomous Robots</title>
  <style>
    .code-container {
            display: flex;
            gap: 20px; /* Adjust gap as needed */
            justify-content: center; /* Center the code blocks */
        }
    .code-block {
        display: flex;
        flex-direction: column;
        align-items: flex-start;
    }
    pre {
        text-align: left;
        margin: 0;
        padding: 10px;
        border: 1px solid #ddd;
        background-color: #f9f9f9;
        width: 50em; /* Adjust width as needed */
        box-sizing: border-box;
    }
  </style>


  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src=" "></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.jpeg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href=" ">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">LaNMP: A Language-Conditioned Mobile Manipulation Benchmark for Autonomous Robots</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="http://ajaafar.com">Ahmed Jaafar</a><sup>1</sup>,
            <span class="author-block">
              <a href="https://shreyasraman.netlify.app/">Shreyas Sundara Raman</a><sup>1</sup>,
            </span>
            <span class="author-block">
             <a href="https://github.com/waymao">Yichen Wei</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/sofia-juliani/">Sofia Juliani</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/annekewernerfelt/">Anneke Wernerfelt</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=OM1hDLcAAAAJ&hl=en">Ifrah Idrees</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://jasonxyliu.github.io/">Jason Xinyu Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://cs.brown.edu/people/stellex/">Stefanie Tellex</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Brown University,</span>
            <span class="author-block"><sup>2</sup>Rutgers University,</span>
            <span class="author-block"><sup>3</sup>University of Pennsylvania</span>
          </div>
          
          <div>
            <p style="color:red;">Under Review for NeurIPS Datasets & Benchmarks Track 24 (Single Blind)</p>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1WrXeRNrtur26o2s4jT4M4vT0IsOxcqug/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>RSS24 Workshop Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href= "https://drive.google.com/file/d/13bTWl3Zo6r71Ez6bCLLR_qDe0T7e4ezF/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-image"></i>
                  </span>
                  <span>RSS24 Workshop Poster</span>
                  </a>
              </span>
              <span class="link-block">
                <a href=" "
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (Coming Soon)</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://youtu.be/IKfcDu_4k4o"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/h2r/NPM-Dataset"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://www.dropbox.com/scl/fo/c1q9s420pzu1285t1wcud/AGMDPvgD5R1ilUFId0i94KE?rlkey=7lwmxnjagi7k9kgimd4v7fwaq&dl=0"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <div style="display: flex; justify-content: center; align-items: center; height: 50vh;">
  <iframe width="700" height="394" src="https://youtu.be/IKfcDu_4k4o" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div> -->

<br>

<div style="display: flex; justify-content: center; align-items: center; height: 50vh;">
  <iframe width="700" height="394" src="https://www.youtube.com/embed/IKfcDu_4k4o?si=BGzt6LK4JtJkdqa0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>



<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->

<br>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            As robots that follow natural language become more capable and prevalent, we need a benchmark to 
            holistically develop and evaluate their ability to solve long-horizon mobile manipulation tasks 
            in large, diverse environments. To tackle this challenge, robots must use visual and language 
            understanding, navigation, and manipulation capabilities. Existing datasets do not integrate all 
            of these aspects, restricting their efficacy as benchmarks.
         </p>
          <p>
            To address this gap, we present the <b>La</b>nguage, <b>N</b>avigation, <b>M</b>anipulation, 
            <b>P</b>erception (<b>LaNMP</b>) dataset and demonstrate the benefits of integrating these 
            four capabilities and various modalities. LaNMP comprises 574 trajectories across eight simulated 
            and real-world environments for long-horizon room-to-room pick-and-place tasks specified by natural language.
            Every trajectory consists of over 20 attributes, including RGB-D images, segmentations, and the poses of the robot 
            body, end-effector, and grasped objects. To demonstrate its efficacy in development and evaluation, we fine-tuned 
            and tested two models in simulation and on a physical robot. The models perform suboptimally compared to humans 
            across various metrics, indicating significant room for developing better multimodal mobile manipulation models using our benchmark.
         </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
<!--     <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<section class="section" style="margin-top: 0; padding-top: 0;">
  <div class="container is-max-desktop" style="margin-top: 0; padding-top: 0;">
    <div class="columns is-centered has-text-centered" style="margin-top: 0; padding-top: 0;">
      <div class="column is-four-fifths" style="margin-top: 0; padding-top: 0;">
        <h2 class="title is-3" style="margin-top: - 5px;">Trajectories</h2>
        <div class="content has-text-justified">
  <div style="text-align: center;">
    <img src="traj_fig.png" alt="Trajectory figure" width="700"> 
  </div>
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
  <div style="text-align: center;">
  <br>
  <h2 class="title is-3">Examples of the Data's Schema</h2>
  <div class="code-container">
    <div class="code-block">
    <p>Simulation - at every time step of the 524 trajectories</p>
    <pre><code>
      {
        "nl_command": "Find the pepper and put it on top of the green chair with a blue pillow on it.",
        "scene": "FloorPlan_Train8_1",
        "steps": [
            {
                "sim_time": 0.19645099341869354,
                "wall-clock_time": "15:49:37.334",
                "action": "Initialize",
                "state_body": [ # robot pose
                    3.0,
                    0.9009992480278015,
                    -4.5,
                    269.9995422363281
                ],
                "state_ee": [ # end-effector pose
                    2.5999975204467773,
                    0.8979992270469666,
                    -4.171003341674805,
                    -1.9440563492718068e-07,
                    -1.2731799533306385,
                    1.9440386333307377e-07
                ],
                "hand_sphere_radius": 0.05999999865889549
                "held_objs": [],
                "held_objs_state": {},
                "inst_det2D": {
                    "keys": [ # identified instances in the environment
                        "Wall_4|0.98|1.298|-2.63",
                        "Wall_3|5.43|1.298|-5.218",
                        "RemoteControl|+01.15|+00.48|-04.24",
                        ...],
                    "values": [ # bounding box coordinates of each instance
                        [418, 43, 1139, 220],
                        [315, 0, 417, 113],
                        [728, 715, 760, 719],
                        ...]
                },
                "rgb": "./rgb_0.npy", # path of visual data for this timestep
                "depth": "./depth_0.npy",
                "inst_seg": "./inst_seg_0.npy",
            }
          ]
       }
    </code></pre>
    </div>
    <div class="code-block">
      <p>Real - at every time step of the 50 trajectories</p>
    <pre><code>
      {
          "language_command": "Take the cup from the table in the dining area which is closest to the stairs and bring it to the table near the
            couches in the corner of the big dining room besides the windows.",
          "scene_name": "upstairs",
          "wall_clock_time": "05:29:40.117",
          "left_fisheye_rgb": "./Trajectories/trajectories/data_33/folder_0.zip/left_fisheye_image_0.npy", # path of visual data for this timestep
          "left_fisheye_depth": "./Trajectories/trajectories/data_33/folder_0.zip/left_fisheye_depth_0.npy",
          "right_fisheye_rgb": "./Trajectories/trajectories/data_33/folder_0.zip/right_fisheye_image_0.npy",
          "right_fisheye_depth": "./Trajectories/trajectories/data_33/folder_0.zip/right_fisheye_depth_0.npy",
          "gripper_rgb": "./Trajectories/trajectories/data_33/folder_0.zip/gripper_image_0.npy",
          "gripper_depth": "./Trajectories/trajectories/data_33/folder_0.zip/gripper_depth_0.npy",
          "left_fisheye_instance_seg": "./Trajectories/trajectories/data_33/folder_0.zip/left_fisheye_image_instance_seg_0.npy",
          "right_fisheye_instance_seg": "./Trajectories/trajectories/data_33/folder_0.zip/right_fisheye_image_instance_seg_0.npy",
          "gripper_fisheye_instance_seg": "./Trajectories/trajectories/data_33/folder_0.zip/gripper_image_instance_seg_0.npy",
          "body_state": {"x": 1.3496176111016192, "y": 0.005613277629761049, "z": 0.15747965011090911},
          "body_quaternion": {"w": 0.04275326839680784, "x": -0.0008884984706659231, "y": -0.00030123853590331847, "z": 0.999085220522855},
          "body_orientation": {"r": -0.003024387647253151, "p": 0.017297610440263775, "y": 3.05395206999625},
          "body_linear_velocity": {"x": 0.00015309476140765987, "y": 0.001022209848280799, "z": 0.0001717336942742603},
          "body_angular_velocity": {"x": 4.532841128101956e-05, "y": 0.003003578426140623, "z": -0.0046712267592016726},
          "arm_state_rel_body": {"x": 0.5535466074943542, "y": -0.00041040460928343236, "z": 0.2611726224422455},
          "arm_quaternion_rel_body": {"w": 0.9999685287475586, "x": -0.0011630485532805324, "y": 0.007775876671075821, "z": 0.007775876671075821},
          "arm_orientation_rel_body": {"x": -0.0023426745301198485, "y": 0.015549442728134426, "z": -0.0021046873064696214},
          "arm_state_global": {"x": 0.7976601233169699, "y": -0.00041040460928343236, "z": 0.2611726224422455},
          "arm_quaternion_global": {"w": 0.043804580215426665, "x": -0.008706641097541701, "y": -0.0011317045101892497, "z": 0.9990015291187636},
          "arm_orientation_global": {"x": -0.003024387647253151, "y": 0.017297610440263775, "z": 3.05395206999625},
          "arm_linear_velocity": {"x": 0.002919594927038712, "y": 0.004658882987521996, "z": 0.012878690074243797},
          "arm_angular_velocity": {"x": -0.01867944403436315, "y": 0.02911512882983833, "z": -0.008279345145765714},
          "arm_stowed": 1, # Boolean
          "gripper_open_percentage": 0.39261579513549805,
          "object_held": 0, # Boolean
          "feet_state_rel_body": [
              {"x": 0.3215886056423187, "y": 0.17115488648414612, "z": -0.5142754912376404},
              {"x": 0.32302412390708923, "y": -0.17028175294399261, "z": -0.5178792476654053},
              {"x": -0.27173668146133423, "y": 0.16949543356895447, "z": -0.5153297185897827},
              {"x": -0.2700275778770447, "y": -0.1685962975025177, "z": -0.5157276391983032}],
          "feet_state_global": [
              {"x": -0.3341075772867149, "y": -0.14278573670828154, "z": -0.5149532673227382},
              {"x": -0.3063631798978494, "y": 0.19752718640765313, "z": -0.518328069669068},
              {"x": 0.25719142551156154, "y": -0.19181889447285838, "z": -0.5149682779363334},
              {"x": 0.2843717159282008, "y": 0.1451830347804529, "z": -0.5151399962832868}],
          "all_joint_angles": {
              "fl.hx": 0.010119102895259857,
              "fl.hy": 0.7966763973236084,
              "fl.kn": -1.576759934425354, ...},
          "all_joint_velocities": {
              "fl.hx": -0.00440612155944109,
              "fl.hy": -0.004167056642472744,
              "fl.kn": -0.007508249022066593, ...}
       }
    </code></pre>
    </div>
</div>
  <!-- <div style="text-align: center;">
    <img src="json_fig.png" alt="JSON figure" width="600"> 
  </div>
  <div style="text-align: center;">
    <img src="json_fig_real.png" alt="JSON figure" width="600"> 
  </div> -->
    <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
  <div style="text-align: center;">
    <br>
  <h2 class="title is-3">Metadata</h2>
    <div class="image123">
      <p>Environment counts, action distribution, and trajectory lengths. </p>
    <div class="imgContainer">
        <img src="traj_2x2.png" width="700em"/>
    </div>
</div>
    
<!--   <div style="text-align: center;">
    <img src="annie_1.png" alt="JSON figure" width="300"> 
  </div>
  <div style="text-align: center;">
    <img src="annie_2.png" alt="JSON figure" width="300"> 
  </div>
  <div style="text-align: center;">
    <img src="annie_3.png" alt="JSON figure" width="300"> 
  </div>
  <div style="text-align: center;">
    <img src="real_traj.png" alt="JSON figure" width="300">  -->
  </div>
  </div>
  </div>
  </div>
  </div>
      </div>
    </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Real-World Maps</h2>
        <div class="content has-text-justified">
          <p>The maps of the environments made by the robot to be used for data collection. On the left is the 2D visualization and on the right is a 3D point cloud of the environment. The top row depicts the laboratory and the bottom row depicts the multi-floor university building environments.</p>
  <div style="text-align: center;">
    <img src="lab_maps.png" alt="Lab maps figure" width="700em"> 
  </div>
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
</section>


<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
    <h2 class="title is-3">Acknowledgements</h2>
    <div style="max-width: 40em; margin: 0 auto;"> <!-- Adjust the max-width as needed -->
      <p>
        This work is supported by ONR under grant award numbers N00014-22-1-2592 and N00014-23-1-2794, NSF under grant award number CNS-2150184, and with support from Amazon Robotics.
    We also thank Aryan Singh, George Chemmala, Ziyi Yang, David Paulius, Ivy He, Lakshita Dodeja, Mingxi Jia, Benned Hedegaard, Thao Nguyen, Selena Williams, Benedict Quartey, Tuluhan Akbulut, and George Konidaris for their help in various phases of work.
      </p>
    </div>
  </div>
</div>
</section>


<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
  <h2 class="title is-3">BibTeX</h2>
<div class="code-container">
  <div class="code-block">
  <pre><code>
    Coming Soon!
  </code></pre>
</div>
</div>
    </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            The <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website is from nerfies.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>