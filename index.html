<!DOCTYPE html>
<html>
<head>
  <link rel="icon" href="favicon.ico" type="image/x-icon">
  <link rel="stylesheet" href="index.css">
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Dataset, Language, Navigation, Perception, Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>λ: A Benchmark for Data-Efficiency in Long-Horizon Indoor Mobile Manipulation Robotics</title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">λ: A Benchmark for Data-Efficiency in Long-Horizon Indoor Mobile Manipulation Robotics</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="http://ajaafar.com">Ahmed Jaafar</a><sup>1</sup>,
            <span class="author-block">
              <a href="https://shreyasraman.netlify.app/">Shreyas Sundara Raman</a><sup>1</sup>,
            </span>
            <span class="author-block">
             <a href="https://github.com/waymao">Yichen Wei</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://sudarshan-s-harithas.github.io/">Sudarshan Harithas</a><sup>1</sup>,
             </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/sofia-juliani/">Sofia Juliani</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/annekewernerfelt/">Anneke Wernerfelt</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://benedictquartey.github.io/">Benedict Quartey</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=OM1hDLcAAAAJ&hl=en">Ifrah Idrees</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://jasonxyliu.github.io/">Jason Xinyu Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://cs.brown.edu/people/stellex/">Stefanie Tellex</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Brown University,</span>
            <span class="author-block"><sup>2</sup>Rutgers University,</span>
            <span class="author-block"><sup>3</sup>University of Pennsylvania</span>
          </div>
          
          <div>
            <a href="https://www.iros25.org/">
              <p style="color:white; font-size: 24px;"><b>IROS 2025</b></p>
            </a>
            
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href= ""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-image"></i>
                  </span>
                  <span>Poster</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2412.05313"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=CJL5X23rq34"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> 
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/h2r/NPM-Dataset"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://www.dropbox.com/scl/fo/c1q9s420pzu1285t1wcud/AGMDPvgD5R1ilUFId0i94KE?rlkey=7lwmxnjagi7k9kgimd4v7fwaq&dl=0"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<br>

<!-- Video -->
<div style="display: flex; justify-content: center; align-items: center; height: auto; margin-bottom: 4vh; margin-top: 4vh;">
  <iframe style="width: 40%; height: 23vw;" src="https://www.youtube.com/embed/CJL5X23rq34?si=bwbidpeJhDu4p88R" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>

<section class="section">
  <div style="margin-top: -5vh;"></div>
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Efficiently learning and executing long-horizon mobile manipulation (MoMa) tasks is crucial for advancing robotics in household and workplace settings. However, current MoMa models are data-inefficient, underscoring the need for improved models that require realistic-sized benchmarks to evaluate their efficiency, which do not exist. To address this, we introduce the <b>LAMBDA (λ)</b> benchmark (Long-horizon Actions for Mobile-manipulation Benchmarking of Directed Activities), which evaluates the data efficiency of models on language-conditioned, long-horizon, multi-room, multi-floor, pick-and-place tasks using a dataset of manageable size, more feasible for collection. The benchmark includes <b>571</b> human-collected demonstrations that provide realism and diversity in simulated and real-world settings. Unlike planner-generated data, these trajectories offer natural variability and replay-verifiability, ensuring robust learning and evaluation. We benchmark several models, including learning-based models and a neuro-symbolic modular approach combining foundation models with task and motion planning. Learning-based models show suboptimal success rates, even when leveraging pretrained weights, underscoring significant data inefficiencies. However, the neuro-symbolic approach performs significantly better while being more data efficient. Findings highlight the need for more data-efficient learning-based MoMa approaches. λ addresses this gap by serving as a key benchmark for evaluating the data efficiency of those future models in handling household robotics tasks.
         </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section style="background-color: #bb8d8d; padding: 2rem;">
<section class="section" style="margin-top: 0; padding-top: 0;">
  <div class="container is-max-desktop" style="margin-top: 0; padding-top: 0;">
    <div class="columns is-centered has-text-centered" style="margin-top: 0; padding-top: 0;">
      <div class="column is-four-fifths" style="margin-top: 0; padding-top: 0;">
        <h2 class="title is-3" style="margin-top: - 5px;">Overview</h2>
        <div class="content has-text-justified">
  <div style="text-align: center;">
    <img src="lambda_cover.png" alt="Overview figure"               
    style="
    display: inline-block;        /* for text-align centering */
    transform: scale(1.25);       /* make it 125% as big */
    transform-origin: top center; /* grow *downward* only */
    margin-bottom: 7rem;
  ">
  </div>
  
  <br>

  <div class="content has-text-justified">
    <p>
      Improving data efficiency for long-horizon mobile manipulation (MoMa) tasks is critical for practical robotic deployment in human environments. Current approaches demand large-scale datasets, which are costly and resource-intensive. To bridge this gap, we introduce the LAMBDA (λ) benchmark, a dataset with 571 language-conditioned, human-collected demonstrations covering diverse indoor multi-room, multi-floor pick-and-place tasks. Unlike existing large datasets, λ emphasizes data efficiency, realistic variability, and replay-verifiability, serving as a valuable testbed for developing robust, practical MoMa models.
   </p>

   <br>

   <div style="text-align: center;">
    <img src="rw.png" alt="Related works figure" 
    style="
    display: inline-block;        /* for text-align centering */
    transform: scale(1.25);       /* make it 125% as big */
    transform-origin: top center; /* grow *downward* only */
    margin-bottom: 6rem;
  ">
  </div>

  <br>

   <p>
    Existing benchmarks for MoMa robotics often lack critical elements necessary for real-world applicability, such as natural language conditioning, long-horizon tasks, human-collected demonstrations, and real-world validation. Most benchmarks either rely on planner-generated data, templated commands, or are restricted to tabletop manipulation, limiting their realism and utility. Very few benchmarks offer free-form natural language instructions, quadruped robot data, or multi-room/floor navigation. The λ benchmark uniquely integrates all these elements, addressing significant gaps and providing a comprehensive evaluation framework for realistic, long-horizon MoMa tasks.
   </p>
  </div>

  <br>
  <br>
  <br>

  <section class="section" style="margin-top: 0; margin-bottom: 0; padding-top: 0;">
    <div class="container is-max-desktop" style="margin-top: 0; padding-top: 0;">
      <div class="columns is-centered has-text-centered" style="margin-top: 0; padding-top: 0;">
        <div class="column is-four-fifths" style="margin-top: 0; padding-top: 0;">
          <h2 class="title is-3" style="margin-top: -5px;">Demonstrations</h2>
          <div class="content has-text-justified">
  
            <!-- first image stays at 80vw -->
            <div style="text-align: center;">
              <img
                src="traj_fig.png"
                alt="Trajectory figure"
                style="
                  display: inline-block;        /* for text-align centering */
                  transform: scale(1.25);       /* make it 125% as big */
                  transform-origin: top center; /* grow *downward* only */
                  margin-bottom: 6rem;
                ">
            </div>
            
            <br>
  
            <!-- second image bumped to 100vw (wider than the first) -->
            <div style="text-align: center; margin-top: 1rem; overflow: visible;">
              <img
                src="traj_split_table.png"
                alt="Second demonstration figure"
                style="width:45vw; max-width:none; height:auto; display:block; position:relative; left:50%; transform:translateX(-50%);"
              >
            </div>

            <br>

            <p>
              The λ benchmark assesses models' data efficiency specifically for long-horizon mobile manipulation tasks involving language-conditioned, room-to-room, and floor-to-floor pick-and-place activities. It comprises 571 expert human-collected demonstrations, blending simulated data (521 trajectories) with real-world data (50 trajectories), reflecting diverse objects, instructions, and realistic complexity. Tasks are specified via crowdsourced free-form natural language instructions, challenging models to generalize robustly across varied linguistic expressions. The use of both simulated environments and real-world data ensures comprehensive and practical evaluation.
            </p>


          </div>
        </div>
      </div>
    </div>
  </section>
  
  
  <br>
  <br>

  <section class="section" style="padding: 0; overflow: visible;">
    <div class="container is-max-desktop" style="padding: 0; overflow: visible;">
            <div class="columns"
           style="padding: 0; overflow: visible; display: flex; justify-content: center;">
        <div class="column" style="padding: 0; overflow: visible;">
  
          <!-- enlarge this from 130% → 200% -->
          <div class="wide-bleed"
               style="
                 position: relative;
                 width: 200%;
                 max-width: none;
                 left: 50%;
                 transform: translateX(-50%);
                 overflow: visible;
               ">
            <h2 class="title is-3"
                style="text-align: center; margin-bottom: 1rem;">
              Results
            </h2>
            </div>
            
            <p>
              To establish baseline performance for λ, we benchmarked two behavior cloning (BC) models: RT-1, a transformer-based MoMa model originally trained on large-scale robot data, and MotionGlot-MoMa, an adapted version of MotionGlot designed for multi-embodiment action generation. Both models were evaluated when trained from scratch and after fine-tuning their pretrained parameters. Additionally, we evaluated LIMP, a zero-shot neuro-symbolic system integrating large multimodal foundation models with task and motion planning, requiring no robot demonstration training. Models were assessed using a success rate metric, where tasks comprise sequential subtasks: navigating to an object, grasping, transporting, and placing it at the goal location, measuring performance comprehensively across long-horizon tasks.
            </p>

            <br>

            <div class="wide-bleed" style="text-align: center; margin-top: 1rem; overflow: visible;">
              <img
                src="scene_gen.png"
                alt="Scene generalization experiment figure"
                style="width:33vw; max-width:none; height:auto; display:block; position:relative; left:50%; transform:translateX(-50%);"
              >
            </div>

            <br>

            <p>
              RT-1 and MG-MoMa achieved low success rates averaging 2.7% and 2.4%, respectively, indicating significant challenges in generalizing to unseen environments with novel room layouts and object placements. Both models marginally surpassed a random baseline, confirming minimal learning. Performance varied slightly across scenes based on complexity and spatial constraints, with simpler tasks achieving slightly higher scores. Overall, the results underscore current limitations in end-to-end models for scene generalization in long-horizon MoMa tasks.
            </p>

            <br>

            <div style="text-align: center; margin-top: 1rem; overflow: visible;">
              <img
                src="task_gen.png"
                alt="Task generalization experiment figure"
                style="width:33vw; max-width:none; height:auto; display:block; position:relative; left:50%; transform:translateX(-50%);"
              >
            </div>
            
            <br>


            <p>
              In task generalization, RT-1 and MG-MoMa exhibited slightly improved but still low success rates, around 5%. Fine-tuning these models with pretrained parameters did not substantially enhance performance, indicating limited benefits from pretraining on external data. Conversely, LIMP, the zero-shot neuro-symbolic system, significantly outperformed end-to-end models with a 44.4% success rate. This result highlights the promise of neuro-symbolic approaches in generalizing to novel tasks without robot-specific demonstration training.
            </p>

     

  
        </div>
      </div>
    </div>
  </section>

  <br>
  <br>
  <br>
  <br>

  <section>
    <div class="container is-max-desktop" style="margin-top: 0; padding-top: 0;">
      <div class="columns is-centered has-text-centered" style="margin-top: 0; padding-top: 0;">
        <div class="column is-four-fifths" style="margin-top: 0; padding-top: 0;">
          <h2 class="title is-3" style="margin-top: 0; margin-bottom: 1rem;">
            Ablations
          </h2>
          <div style="text-align: center; margin-top: 1rem;">
            <img
              src="ablations.png"
              alt="Ablations figure"
              style="
                display: inline-block;
                transform: scale(1.25);
                transform-origin: top center;
              "
            >
          </div>  
        </div>
      </div>
    </div>
  </section>
  
  <div class="container is-max-desktop" style="margin-top: 7rem;">
    <p>
      We investigated how varying dataset sizes and model architectures impact data efficiency in MoMa tasks. Increasing the dataset size from 25% to 100% led to modest performance improvements for RT-1, indicating data inefficiency. Architectural comparisons revealed that replacing RT-1's transformer with a Mamba architecture (RM-1) resulted in consistently better performance across all dataset sizes, outperforming both the transformer and LSTM alternatives. These findings suggest that architectural choices significantly influence data efficiency, with Mamba-based models demonstrating superior generalization capabilities for long-horizon MoMa tasks.
    </p>
  </div>
  

  <br>
  <br>
  <br>

  </section>

  </div>
  </div>
  </div>
  </div>
  </div>
      </div>
    </div>
</section>


<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
    <h2 class="title is-3">Acknowledgements</h2>
    <div style="max-width: 40em; margin: 0 auto;"> <!-- Adjust the max-width as needed -->
       <p>
        This work is supported by ONR under grant award numbers N00014-22-1-2592 and N00014-23-1-2794, NSF under grant award number CNS-2150184, and with support from Amazon Robotics.
    We also thank Aryan Singh, George Chemmala, Ziyi Yang, David Paulius, Ivy He, Lakshita Dodeja, Mingxi Jia, Benned Hedegaard, Thao Nguyen, Selena Williams, Tuluhan Akbulut, and George Konidaris for their help in various phases of work.
      </p>
    </div>
  </div>
</div>
</section>

<br>
<br>
<br>
<br>

<section class="section">
  <div style="margin-top: -11vh;"></div>
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
  <h2 class="title is-3">BibTeX</h2>
<div class="code-container">
  <div class="code-block" style="flex: 1 1 90%; max-width: 65%;">
  <pre><code>
    @misc{lambdabenchmark,
      title={{\lambda}: A Benchmark for Data-Efficiency in Long-Horizon Indoor Mobile Manipulation Robotics}, 
      author={Ahmed Jaafar and Shreyas Sundara Raman and Yichen Wei and Sofia Juliani and Anneke Wernerfelt and Benedict Quartey and Ifrah Idrees and Jason Xinyu Liu and Stefanie Tellex},
      year={2025},
      eprint={2412.05313},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2412.05313}, 
    }
  </code></pre>
</div>
</div>
    </div>
</section>


<footer class="footer" style="padding: 0; display: flex; align-items: center; justify-content: center; text-align: center; background-color: #f5f5f5;">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            The <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website is from nerfies.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
